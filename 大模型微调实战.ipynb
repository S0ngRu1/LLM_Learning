{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f18007-ef8c-4ac8-a6fc-8e3467fb2807",
   "metadata": {},
   "source": [
    "# 大模型微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4094739-1d8c-4281-ad18-f45f0eb172dd",
   "metadata": {},
   "source": [
    "## 安装需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20a2a49-ec57-43ab-a254-aae6b94ef525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T01:12:04.850675Z",
     "iopub.status.busy": "2025-08-15T01:12:04.850304Z",
     "iopub.status.idle": "2025-08-15T01:12:10.251936Z",
     "shell.execute_reply": "2025-08-15T01:12:10.251208Z",
     "shell.execute_reply.started": "2025-08-15T01:12:04.850655Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Collecting swanlab\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/92/5c/aee5fdcaa1b6de64f1f505d66e07d748cfd995948f9bda1edf506d50bf5e/swanlab-0.6.8-py3-none-any.whl (290 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.35.49 in /usr/local/lib/python3.11/site-packages (from swanlab) (1.39.3)\n",
      "Requirement already satisfied: botocore in /usr/local/lib/python3.11/site-packages (from swanlab) (1.39.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: platformdirs>=4.2.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (7.0.0)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.11.7)\n",
      "Collecting pyecharts>=2.0.0 (from swanlab)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/ae/18/383622b338e4f6948ba1b75a8155d748ce097ead08a4163ca763f0ad510e/pyecharts-2.0.8-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pynvml in /usr/local/lib/python3.11/site-packages (from swanlab) (12.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from swanlab) (65.5.1)\n",
      "Collecting swankit==0.2.4 (from swanlab)\n",
      "  Downloading https://mirrors.cloud.aliyuncs.com/pypi/packages/4a/c7/7cc8d6bc562ce96d751a7655421eae09ba795cd557ed4791d63a72bd8f9a/swankit-0.2.4-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.17.0 in /usr/local/lib/python3.11/site-packages (from swanlab) (1.17.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/site-packages (from boto3>=1.35.49->swanlab) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from pyecharts>=2.0.0->swanlab) (3.1.6)\n",
      "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/site-packages (from pyecharts>=2.0.0->swanlab) (3.16.0)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/site-packages (from pyecharts>=2.0.0->swanlab) (3.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.0->swanlab) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.28.0->swanlab) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->swanlab) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich<14.0.0,>=13.6.0->swanlab) (2.19.1)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/site-packages (from pynvml->swanlab) (12.575.51)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->swanlab) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->pyecharts>=2.0.0->swanlab) (3.0.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prettytable->pyecharts>=2.0.0->swanlab) (0.2.13)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: swankit, pyecharts, swanlab\n",
      "Successfully installed pyecharts-2.0.8 swankit-0.2.4 swanlab-0.6.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install swanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520915f-02da-4809-9e19-f3d8bb8aa725",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7a3241-06d1-4af5-ba25-a51293d682ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T07:23:47.566963Z",
     "iopub.status.busy": "2025-08-14T07:23:47.566634Z",
     "iopub.status.idle": "2025-08-14T07:24:09.266959Z",
     "shell.execute_reply": "2025-08-14T07:24:09.266465Z",
     "shell.execute_reply.started": "2025-08-14T07:23:47.566942Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [README.md]: 100%|██████████| 1.33k/1.33k [00:00<00:00, 8.97MB/s]\n",
      "2025-08-14 15:23:57,375 - modelscope - INFO - storing https://www.modelscope.cn/api/v1/datasets/krisfu/delicate_medical_r1_data/repo?Source=SDK&Revision=master&FilePath=README.md&View=False in cache at /mnt/workspace/.cache/modelscope/hub/datasets/06b8db8bcdfac6b78df6a26d6969f81867cc1311bc9ce4384b5fb1cffc9504c9\n",
      "2025-08-14 15:23:57,391 - modelscope - INFO - creating metadata file for /mnt/workspace/.cache/modelscope/hub/datasets/06b8db8bcdfac6b78df6a26d6969f81867cc1311bc9ce4384b5fb1cffc9504c9\n",
      "Downloading data: 100%|██████████| 9.20M/9.20M [00:00<00:00, 10.9MB/s]\n",
      "2025-08-14 15:24:07,917 - modelscope - INFO - storing https://www.modelscope.cn/api/v1/datasets/krisfu/delicate_medical_r1_data/repo?Source=SDK&Revision=master&FilePath=r1_data_example.jsonl in cache at /mnt/workspace/.cache/modelscope/hub/datasets/downloads/01b7be597488efa04be3c3838a6cb36d08f9b1e99874a112d52790dd4050ad78\n",
      "2025-08-14 15:24:07,932 - modelscope - INFO - creating metadata file for /mnt/workspace/.cache/modelscope/hub/datasets/downloads/01b7be597488efa04be3c3838a6cb36d08f9b1e99874a112d52790dd4050ad78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868d8b99e3714f33889a4cdc8b834f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been split successfully.\n",
      "Train Set Size：2166\n",
      "Val Set Size：241\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "print(f\"The dataset is loading.\")\n",
    "ds = MsDataset.load('krisfu/delicate_medical_r1_data', subset_name='default', split='train')\n",
    "data_list = list(ds)\n",
    "random.shuffle(data_list)\n",
    "\n",
    "split_idx = int(len(data_list) * 0.9)\n",
    "\n",
    "train_data = data_list[:split_idx]\n",
    "val_data = data_list[split_idx:]\n",
    "\n",
    "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "print(f\"The dataset has been split successfully.\")\n",
    "print(f\"Train Set Size：{len(train_data)}\")\n",
    "print(f\"Val Set Size：{len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e741c9-60c2-4163-b30c-2cf33aa8b884",
   "metadata": {},
   "source": [
    "## 查看数据样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a0f684-5a3d-46de-85e8-9bcd2d7a23f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T07:26:31.939914Z",
     "iopub.status.busy": "2025-08-14T07:26:31.939486Z",
     "iopub.status.idle": "2025-08-14T07:26:31.990495Z",
     "shell.execute_reply": "2025-08-14T07:26:31.990085Z",
     "shell.execute_reply.started": "2025-08-14T07:26:31.939894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '说明测定盐酸甲氧明注射液含量的具体步骤。',\n",
       " 'question': '医生，我想了解一下如何测定盐酸甲氧明注射液的含量，具体步骤是什么样的？',\n",
       " 'think': '嗯，用户问的是如何测定盐酸甲氧明注射液的含量，具体步骤是什么。首先，我需要回忆一下相关的分析方法，特别是药典中常用的方法。盐酸甲氧明属于药物分析中的常见成分，通常可能用紫外分光光度法或者HPLC之类的。\\n\\n首先，用户提到的是紫外-可见分光光度法，这应该是比较直接的方法。那步骤的话，首先应该考虑样品的制备。因为注射液浓度可能较高，需要稀释到合适的浓度范围，使得吸光度在仪器的线性范围内。比如，先取一定量的注射液，然后稀释到某个体积，比如250毫升的量瓶，这样可能是一个步骤。\\n\\n接下来，可能需要进一步稀释，因为第一次稀释后的溶液可能吸光度还是太高或者太低。比如，从第一次稀释后的溶液中再取一部分，比如10毫升，放到另一个量瓶，比如100毫升，再次稀释。这样两次稀释可以得到合适的浓度。\\n\\n然后，测定吸光度的时候，波长的选择很重要。盐酸甲氧明的最大吸收波长应该查文献或者药典，比如问题中的答案提到的是290纳米，这可能来自药典的规定。需要确认这个波长是否正确，以及吸收系数是否准确。比如，吸收系数137是否对应C1H1NO3·HCl的计算，可能需要根据分子量和浓度来计算，但用户可能不需要详细计算，只需要步骤。\\n\\n另外，步骤中的精密量取需要注意，比如使用移液管或者自动进样器，确保量取的准确性。摇匀也是关键步骤，确保溶液均匀，避免局部浓度不均导致误差。\\n\\n可能还需要考虑空白对照，比如用水代替样品，进行同样的操作，以扣除背景吸光度。不过答案中没有提到，可能在标准方法中已经包含，或者步骤简化了。但作为详细步骤，可能需要补充这一点。\\n\\n最后，计算含量的时候，根据吸收系数和吸光度，应用朗伯-比尔定律，A=εcl，其中ε是摩尔吸光系数，但这里可能用的是百分吸收系数（1% 1cm），所以需要确认单位是否正确。例如，吸收系数137可能是指在1%浓度（1g/100ml）下，1cm光程的吸光度，这样计算时需要将浓度转换为相应的单位。\\n\\n可能用户是医生或者药剂师，需要准确的步骤来执行或理解检测流程，所以步骤要清晰，符合药典标准。需要确保步骤中的每个操作都有依据，比如量取的体积、稀释倍数、波长选择等，都符合药典推荐的方法，这样结果才可靠。\\n\\n总结下来，步骤应该是：取样、两次稀释、测定吸光度、计算含量。同时要强调方法的准确性和药典推荐，让用户知道这是标准方法，可信度高。\\n',\n",
       " 'answer': '您好，测定盐酸甲氧明注射液的含量通常采用紫外-可见分光光度法。首先，需要精密量取适量的注射液（大约相当于100毫克盐酸甲氧明），将其置于250毫升的量瓶中，并用水稀释至刻度线，充分摇匀。然后，从这个溶液中精密量取10毫升，放入100毫升的量瓶中，再次用水稀释至刻度线，摇匀。最后，使用紫外-可见分光光度计，在290纳米的波长处测定吸光度，根据C1H1NO3·HCl的吸收系数137来计算盐酸甲氧明的含量。这种方法准确可靠，是药典推荐的测定方法。',\n",
       " 'metrics': {'quality_f1': 1.0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('train.jsonl') as f:\n",
    "  content = f.readlines()\n",
    "\n",
    "datas = []\n",
    "for data in content:\n",
    "  datas.append(json.loads(data))\n",
    "\n",
    "datas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035ce38-2eba-405a-88fc-aabd9d734791",
   "metadata": {},
   "source": [
    "## 加载模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18128b59-20af-4b99-878e-09db33aa0934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T07:27:00.650734Z",
     "iopub.status.busy": "2025-08-14T07:27:00.650436Z",
     "iopub.status.idle": "2025-08-14T07:28:21.503508Z",
     "shell.execute_reply": "2025-08-14T07:28:21.503052Z",
     "shell.execute_reply.started": "2025-08-14T07:27:00.650715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:27:03.855289: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-14 15:27:04.129664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-14 15:27:05.324863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: ./Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:27:07,980 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "2025-08-14 15:27:08,158 - modelscope - INFO - Got 12 files, start to download ...\n",
      "Processing 12 items:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1b9684463b457284ac6df5101157a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf1d50b1fd04e22850b585229cf0bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00002-of-00002.safetensors]:   0%|          | 0.00/594M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9d0b902afd4c61beed2111524e0137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors.index.json]:   0%|          | 0.00/25.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e6e8e9650d4a06818bb708d2535665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00001-of-00002.safetensors]:   0%|          | 0.00/3.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c476dc01e6ea4d0595d9777825764f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d565e67407e44b695bb80917db05baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6ff6293de242a3935b9d25bcaf4440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a42ed28aab405caba9ef55b3c4b119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 12 items:   8%|▊         | 1/12 [00:01<00:13,  1.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9293b250c7943fcb461e8ef642f971b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aae9963dee54c4b8a88fde14a648678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/10.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 12 items:  25%|██▌       | 3/12 [00:01<00:03,  2.84it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e2b26e0e9244e28a08a9646da7b548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e60f68b4814ea185a3aeaea1c3757d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 12 items: 100%|██████████| 12/12 [00:27<00:00,  2.31s/it]\n",
      "2025-08-14 15:27:35,827 - modelscope - INFO - Download model 'Qwen/Qwen3-1.7B' successfully.\n",
      "2025-08-14 15:27:35,829 - modelscope - INFO - Creating symbolic link [./Qwen/Qwen3-1.7B].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f821e5a9a5b4416080393025786ab102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# 在modelscope上下载Qwen模型到本地目录下\n",
    "model_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"./\", revision=\"master\")\n",
    "\n",
    "# Transformers加载模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec3c672-f8a3-462c-8795-03244c9d7cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:25:10.456853Z",
     "iopub.status.busy": "2025-08-14T08:25:10.456539Z",
     "iopub.status.idle": "2025-08-14T08:25:21.705446Z",
     "shell.execute_reply": "2025-08-14T08:25:21.704920Z",
     "shell.execute_reply.started": "2025-08-14T08:25:10.456833Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m You can find your API key at: \u001b[33mhttps://swanlab.cn/space/~/settings\u001b[0m\n",
      "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Paste an API key from your profile and hit enter, or press \u001b[32m'CTRL + C'\u001b[0m \n",
      "to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!swanlab login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4d84e0-f506-45d4-92a9-9ebd2517c97d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-15T01:15:41.291351Z",
     "iopub.status.busy": "2025-08-15T01:15:41.290891Z",
     "iopub.status.idle": "2025-08-15T02:32:07.763618Z",
     "shell.execute_reply": "2025-08-15T02:32:07.763093Z",
     "shell.execute_reply.started": "2025-08-15T01:15:41.291332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Tracking run with swanlab version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Tracking run with swanlab version \u001b[1;36m0.6\u001b[0m.\u001b[1;36m8\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Run data will be saved locally in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/mnt/workspace/swanlog/run-20250815_091543-n4nb1emz9pcnw4xeomu4c</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Run data will be saved locally in \u001b[1;35m/mnt/workspace/swanlog/run-20250815_091543-n4nb1emz9pcnw4xeomu4c\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> </span>👋 Hi <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">csr</span>,welcome to swanlab!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m\u001b[1;34m \u001b[0m👋 Hi \u001b[1;39mcsr\u001b[0m,welcome to swanlab!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> Syncing run <span style=\"color: #808000; text-decoration-color: #808000\">rat-2</span> to the cloud\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m Syncing run \u001b[33mrat-2\u001b[0m to the cloud\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@csr/qwen3-sft-medical-LORA</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@csr/qwen3-sft-medical-LORA\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@csr/qwen3-sft-medical-LORA/runs/n4nb1emz9pcnw4xeomu4c</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@csr/qwen3-sft-medical-LORA/runs/n4nb1emz9pcnw4xeomu4c\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@csr/qwen3-sft-medical-LORA/runs/n4nb1emz9pcnw4xeomu4c\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 09:15:45,842 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc1d96abcec4366932e321381892981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,716,288 || all params: 1,729,291,264 || trainable%: 0.5040\n",
      "加载训练集并进行预处理...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ad1228e31e43c9838eab493edb1059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载验证集并进行预处理...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a56ee0b22042b1824f058b7d158803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 09:17:37,036] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py:18: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead\n",
      "  import distutils.sysconfig\n",
      "df: /root/.triton/autotune: 没有那个文件或目录\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 09:17:38,953] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1084' max='1084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1084/1084 1:13:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.291200</td>\n",
       "      <td>1.269640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.225200</td>\n",
       "      <td>1.234406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.209800</td>\n",
       "      <td>1.216754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.191600</td>\n",
       "      <td>1.205671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.187800</td>\n",
       "      <td>1.196303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.154500</td>\n",
       "      <td>1.190607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.153700</td>\n",
       "      <td>1.185939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.182542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.122800</td>\n",
       "      <td>1.181476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.128100</td>\n",
       "      <td>1.180115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存训练好的LoRA适配器...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309f04d69c6542e2869c5ce385a8ab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上进行预测...\n",
      "\n",
      "            样本 1:\n",
      "            问题: 1895年德国物理学教授伦琴的发现对医学影像学的发展有何具体影响？请从技术进步、学科建立和临床应用三个方面进行分析。\n",
      "            模型回答: <|FunctionCallBegin|>嗯，用户问的是1895年伦琴发现X射线对医学影像学的影响，需要从技术进步、学科建立和临床应用三个方面来分析。首先，我得先回忆一下X射线的基本知识，伦琴确实是在1895年发现的，这可能就是用户提到的那一年。然后，我需要分三个部分来展开思考。\n",
      "\n",
      "首先，技术进步方面，X射线是第一种基于辐射的影像技术，对吧？所以这应该算是技术上的突破。之前可能都是通过光学显微镜或者X光片的其他方法，而X射线的发现让医生可以不用显微镜就能看到内部结构，这应该是技术上的重大进展。另外，可能还有后续的技术发展，比如X射线的进一步应用，比如数字影像之类的，但用户的问题可能只需要1895年的直接影响，所以可能不需要太深入后续发展，但得提到它作为基础。\n",
      "\n",
      "然后是学科建立方面，伦琴的发现应该促使医学影像学成为独立学科。之前可能更多是解剖学或者外科的分支，而影像学作为独立学科，需要系统的研究和理论发展。比如，可能成立了专门的医学影像学机构，或者培养相关人才，推动理论体系的建立。另外，可能还有相关学科如放射学、内窥镜等的兴起，都是建立在X射线基础上的。\n",
      "\n",
      "临床应用方面，X射线的直接应用就是诊断疾病，比如骨折、肺结节等。这使得医生能够更早发现疾病，进行及时治疗。比如，肺部疾病可以通过X光拍片诊断，这改变了传统的诊断方法，比如依赖症状和体征，或者需要手术检查。此外，可能还有后续的发展，比如CT、MRI等，但同样，1895年的发现直接推动了这些技术的出现，作为基础技术。\n",
      "\n",
      "不过，用户可能需要更具体的例子，比如在学科建立方面，伦琴的工作如何引发其他科学家的研究，比如后来的医生和科学家如何将X射线应用到医学中，建立理论，或者形成教学体系。比如，可能在柏林大学设立医学影像学的课程，或者有专门的教材和研究机构。\n",
      "\n",
      "另外，临床应用部分，除了诊断疾病，可能还有其他应用，比如治疗手段，比如X射线引导下的手术，或者治疗某些疾病，但可能主要还是诊断。不过需要确认这一点是否准确，是否有其他应用。\n",
      "\n",
      "还要注意时间点，用户明确提到1895年，所以需要强调这是第一次，其他技术如CT、MRI是在之后发展的，但作为技术进步的基础。学科建立方面，可能需要提到伦琴的工作如何被其他科学家发展，比如后来的放射学之父，或者医学影像学的奠基人，如何推动学科发展。\n",
      "\n",
      "可能需要检查是否有遗漏的方面，比如是否还有其他技术进步，比如X射线的改进，或者后续的设备发展，但问题明确要求是1895年的影响，所以重点在那一年的发现带来的变化。\n",
      "\n",
      "总结一下，技术进步是X射线作为第一种技术，学科建立是作为独立学科，临床应用是诊断疾病的应用。需要将这三个方面逐一展开，每个部分举出具体的例子或机制，比如技术上如何改变诊断方法，学科上如何形成理论体系，临床应用如何改变医疗流程等。\n",
      "<|FunctionCallEnd|> \n",
      " 1895年，德国物理学教授威廉·伦琴的发现X射线对医学影像学的发展具有深远的影响。首先，X射线的发现为医学影像学提供了新的技术手段，它能够穿透人体组织，显示内部结构，这极大地提高了诊断的准确性。其次，这一发现促进了医学影像学作为独立学科的建立，为后续的影像技术发展奠定了基础。最后，X射线的应用使得医生能够更早地发现疾病，从而改变了疾病的诊断和治疗方法。\n",
      "            \n",
      "\n",
      "            样本 2:\n",
      "            问题: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n",
      "            模型回答: <|FunctionCallBegin|>嗯，用户问的是抗溃疡药物的分类、作用机制以及它们如何影响胃黏膜的保护与损伤平衡。首先，我需要回忆一下常见的抗溃疡药物，记得分类的话可能分为质子泵抑制剂、H2受体拮抗剂、黏膜保护剂和促动力药这些类别。然后每个类别里有哪些具体的药物呢？\n",
      "\n",
      "质子泵抑制剂，比如奥美拉唑，是通过抑制质子泵来减少胃酸分泌的，所以应该属于酸分泌抑制剂。H2受体拮抗剂，比如雷尼替丁，是阻止组胺与H2受体结合，减少胃酸。黏膜保护剂比如硫糖铝，可能促进黏液分泌或者形成保护层。促动力药比如多潘立酮，刺激胃蠕动，减少胃酸分泌或者促进排空？\n",
      "\n",
      "接下来要解释每个类别的作用机制。比如质子泵抑制剂的作用机制是直接抑制质子泵，减少HCl的分泌，从而降低胃酸，保护黏膜。H2受体拮抗剂通过阻断组胺的作用，同样减少胃酸。黏膜保护剂可能促进黏液的生成或者形成保护屏障，比如硫糖铝可能促进黏液分泌，形成保护层，同时中和胃酸。促动力药通过加快胃排空，减少胃酸反流的机会。\n",
      "\n",
      "然后要考虑这些药物如何影响胃黏膜的保护与损伤平衡。比如，如果胃酸过多，这些药物通过抑制酸分泌，帮助恢复黏膜的保护状态。如果黏膜本身有损伤，保护剂可能帮助修复，但如果有持续损伤，可能需要其他药物。促动力药可能帮助减少胃内容物滞留，减少对黏膜的刺激？\n",
      "\n",
      "用户提到自己胃部不适，可能需要根据症状选择合适的药物。比如如果症状是胃酸过多，可能质子泵抑制剂更有效；如果症状是反酸或胃胀，可能促动力药。但需要提醒用户不能自行用药，必须遵医嘱。\n",
      "\n",
      "有没有遗漏的分类？比如还有抗酸剂，比如铝碳酸镁，可能属于黏膜保护剂的一种？或者是否应该包括其他类别？比如H2受体拮抗剂和黏膜保护剂可能属于不同的机制，但分类可能需要更明确。\n",
      "\n",
      "另外，作用机制部分是否需要更详细的解释？比如质子泵抑制剂的作用靶点，H2受体拮抗剂的作用靶点，黏膜保护剂的机制，比如中和酸、促进黏液生成等。促动力药的作用是增加胃排空，减少胃酸反流或减少胃内容物对黏膜的直接接触。\n",
      "\n",
      "可能还需要提到这些药物的协同作用，比如同时使用H2受体拮抗剂和黏膜保护剂可能更有效。但用户的问题可能不需要这么深入，但作为回答的一部分，可以简要提及。\n",
      "\n",
      "最后，确保语言通俗易懂，避免专业术语过多，但用户是医生，可能需要稍微专业一点，但问题中的用户是患者，所以需要解释清楚。比如硫糖铝的作用机制是促进黏液分泌，形成保护层，同时中和胃酸，减少刺激。\n",
      "\n",
      "总结下来，结构应该是先分类，每个类别举例子，然后说明每个药物的作用机制，最后讲它们如何影响黏膜的保护与损伤平衡。可能还要提到不同药物的适用情况，比如胃酸过多用PPIs，反酸用H2受体拮抗剂，黏膜保护用硫糖铝，促动力用多潘立酮。但需要根据问题中的答案来组织，答案里已经提到这些药物的分类和作用机制，所以需要按照答案的结构来组织思考过程，确保覆盖所有要点。\n",
      "<|FunctionCallEnd|> \n",
      " 当然可以。抗溃疡药物主要分为几大类，每种药物都有其特定的作用机制和使用范围。第一类是酸分泌抑制剂，这类药物通过抑制胃酸的分泌来治疗胃溃疡。例如，奥美拉唑是一种质子泵抑制剂，它能有效抑制胃酸的分泌，从而减少胃酸对胃黏膜的刺激。第二类是H2受体拮抗剂，它们通过阻断胃酸分泌过程中的关键步骤来减少胃酸，比如雷尼替丁。第三类是黏膜保护剂，这类药物能促进胃黏膜的修复，例如硫糖铝，它有助于形成一层保护性的黏膜屏障，减少胃酸对黏膜的损伤。第四类是促动力药，这类药物通过刺激胃部蠕动，帮助胃内容物更快地排出，减少胃酸反流的可能性，如多潘立酮。每种药物都有其独特的治疗效果，医生会根据您的具体病情选择最合适的药物。\n",
      "            \n",
      "\n",
      "            样本 3:\n",
      "            问题: 我最近被诊断出患有淋巴瘤，医生提到这可能导致发热。请问这是由于淋巴瘤组织的坏死和细胞破坏引起的吗？如果是，具体机制是什么？\n",
      "            模型回答: <|FunctionCallBegin|>嗯，用户问的是淋巴瘤组织坏死和细胞破坏是否会导致发热，以及具体机制是什么。首先，我需要回忆一下淋巴瘤的基本知识。淋巴瘤是起源于淋巴系统的恶性肿瘤，通常分为B细胞和T细胞淋巴瘤。坏死可能是因为肿瘤组织生长迅速，导致周围组织的破坏，进而引发炎症反应。\n",
      "\n",
      "接下来，发热通常由感染或炎症引起，但淋巴瘤患者常见的发热原因可能包括感染、肿瘤本身导致的炎症反应，或者肿瘤坏死释放的物质。坏死组织释放的细胞因子，比如TNF-α、IL-6等，这些物质会引发炎症反应，导致发热。同时，坏死组织本身也可能直接刺激免疫系统，导致发热。\n",
      "\n",
      "另外，细胞破坏可能涉及肿瘤细胞的死亡，释放细胞内容物，这些物质同样可以引发炎症和发热。还有可能肿瘤坏死导致局部组织的直接刺激，比如释放的蛋白酶或其他介质，导致周围组织反应。\n",
      "\n",
      "需要确认这些机制是否正确。比如，TNF-α确实与炎症反应相关，而细胞因子的释放确实会导致发热。此外，感染性发热和肿瘤坏死都属于非感染性发热的原因之一，所以两者都需要考虑。\n",
      "\n",
      "用户提到医生说可能引起发热，所以需要解释机制时要具体到坏死和细胞破坏。可能还要提到治疗上的建议，比如抗感染、退烧药物，但用户的问题主要关注机制，所以重点放在坏死和细胞破坏的作用上。\n",
      "\n",
      "有没有可能遗漏了其他机制？比如，淋巴瘤细胞本身坏死释放的细胞因子，或者坏死组织中的细菌感染？不过通常发热更多是由于炎症介质而非细菌感染。需要明确回答是坏死组织本身的细胞因子和炎症反应，而不是感染。\n",
      "\n",
      "总结下来，答案应该包括：是的，坏死导致炎症因子释放，引发发热；具体机制是坏死组织释放细胞因子如TNF-α、IL-6，导致炎症反应和发热。同时，治疗可能包括退烧药物和控制感染。需要确保这些信息准确，并且用通俗的语言解释，让用户理解原因。\n",
      "<|FunctionCallEnd|> \n",
      " 是的，您的情况可能是由于淋巴瘤组织的坏死和细胞破坏引起的发热。这种发热通常是因为肿瘤组织内的细胞被破坏后释放的炎症介质，如细胞因子等，导致局部或全身的炎症反应。这些炎症介质可以激活免疫系统，引发发热。建议您保持良好的休息和饮食，必要时使用退烧药物。如果有发热的情况，及时与医生沟通，以便采取适当的治疗措施。\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🏠 View project at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@csr/qwen3-sft-medical-LORA</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🏠 View project at \u001b[4;34mhttps://swanlab.cn/@csr/qwen3-sft-medical-LORA\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">swanlab</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">:</span> 🚀 View run at <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">https://swanlab.cn/@csr/qwen3-sft-medical-LORA/runs/n4nb1emz9pcnw4xeomu4c</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mswanlab\u001b[0m\u001b[1;39m:\u001b[0m 🚀 View run at \u001b[4;34mhttps://swanlab.cn/@csr/qwen3-sft-medical-LORA/runs/n4nb1emz9pcnw4xeomu4c\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练和评估完成!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import swanlab\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\n",
    "# 配置项目参数\n",
    "os.environ[\"SWANLAB_PROJECT\"] = \"qwen3-sft-medical-LORA\" \n",
    "PROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# 配置swanlab实验参数\n",
    "run = swanlab.init(\n",
    "    project=os.environ[\"SWANLAB_PROJECT\"],\n",
    "    config={\n",
    "    \"model\": \"Qwen/Qwen3-1.7B-LoRA\", \n",
    "    \"prompt\": PROMPT,\n",
    "    \"data_max_length\": MAX_LENGTH,\n",
    "    \"lora_r\": 8, \n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "def dataset_jsonl_transfer(origin_path, new_path):\n",
    "    \"\"\"\n",
    "    将原始JSONL数据集转换为大模型微调所需的格式\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                output = f\"<|FunctionCallBegin|>{data['think']}<|FunctionCallEnd|> \\n {data['answer']}\"\n",
    "                message = {\n",
    "                    \"instruction\": PROMPT,\n",
    "                    \"input\": data[\"question\"],\n",
    "                    \"output\": output\n",
    "                }\n",
    "                messages.append(message)\n",
    "            except Exception as e:\n",
    "                print(f\"处理数据时出错: {e}, 行内容: {line}\")\n",
    "                \n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    预处理函数：将数据集转换为模型输入格式\n",
    "    \"\"\"\n",
    "    # 构建系统提示和用户输入部分\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # 构建模型回应部分\n",
    "    response = tokenizer(\n",
    "        f\"{example['output']}\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # 合并输入和输出，添加结束标记\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    # labels中对输入部分使用-100表示不参与损失计算\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    \n",
    "    # 截断过长序列\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def predict(messages, model, tokenizer):\n",
    "    \"\"\"\n",
    "    预测函数：根据输入消息生成模型回应\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # 应用聊天模板格式化输入\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 转换为模型输入格式\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 生成回应\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        temperature=0.7,  # 控制生成多样性\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # 提取生成的部分（排除输入部分）\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    # 解码为文本\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "\n",
    "# 主程序执行\n",
    "if __name__ == \"__main__\":\n",
    "    # 下载模型（如果本地没有）\n",
    "    model_dir = snapshot_download(\n",
    "        \"Qwen/Qwen3-1.7B\",\n",
    "        cache_dir=\"/mnt/workspace\",\n",
    "        revision=\"master\"\n",
    "    )\n",
    "    \n",
    "    # 加载tokenizer和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # 对于LoRA微调，如果模型本身没有pad_token，显式指定一个\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # 开启梯度检查点所需配置 \n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    # ==================== 配置和应用LoRA ====================\n",
    "    # 1. 定义LoRA配置\n",
    "    config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # 指定要应用LoRA的模块\n",
    "        r=8,                           # LoRA的秩，常用8, 16, 32。越小参数越少\n",
    "        lora_alpha=16,                 # LoRA的缩放因子，通常是r的两倍\n",
    "        lora_dropout=0.1,              # Dropout比例\n",
    "    )\n",
    "\n",
    "    # 2. 使用get_peft_model将基础模型包装为PEFT模型\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    # 3. 打印可训练参数的数量和比例，以确认LoRA生效\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # 数据集路径配置 \n",
    "    train_dataset_path = \"train.jsonl\"\n",
    "    test_dataset_path = \"val.jsonl\"\n",
    "    train_jsonl_new_path = \"train_format.jsonl\"\n",
    "    test_jsonl_new_path = \"val_format.jsonl\"\n",
    "    \n",
    "    # 转换数据集格式 \n",
    "    if not os.path.exists(train_jsonl_new_path):\n",
    "        print(f\"转换训练集: {train_dataset_path} -> {train_jsonl_new_path}\")\n",
    "        dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n",
    "    \n",
    "    if not os.path.exists(test_jsonl_new_path):\n",
    "        print(f\"转换验证集: {test_dataset_path} -> {test_jsonl_new_path}\")\n",
    "        dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n",
    "    \n",
    "    # 加载并预处理训练集 \n",
    "    print(\"加载训练集并进行预处理...\")\n",
    "    train_df = pd.read_json(train_jsonl_new_path, lines=True)\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n",
    "    \n",
    "    # 加载并预处理验证集\n",
    "    print(\"加载验证集并进行预处理...\")\n",
    "    eval_df = pd.read_json(test_jsonl_new_path, lines=True)\n",
    "    eval_ds = Dataset.from_pandas(eval_df)\n",
    "    eval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)\n",
    "    \n",
    "    # 配置训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/mnt/workspace/Qwen3-1.7B-LoRA\", \n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=4,\n",
    "        save_steps=400,\n",
    "        learning_rate=5e-5,\n",
    "        save_on_each_node=True,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"swanlab\",\n",
    "        run_name=\"qwen3-1.7B-medical-lora\",\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "    )\n",
    "\n",
    "    # 初始化Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    print(\"开始训练...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 保存最终模型\n",
    "    # 对于PEFT模型，只保存适配器部分\n",
    "    print(\"保存训练好的LoRA适配器...\")\n",
    "    final_lora_path = os.path.join(training_args.output_dir, \"final_lora_adapter\")\n",
    "    trainer.model.save_pretrained(final_lora_path)\n",
    "    # 同时也可以保存tokenizer\n",
    "    tokenizer.save_pretrained(final_lora_path)\n",
    "\n",
    "    # ==================== 预测前需要合并模型 ====================\n",
    "    # 在进行预测或部署时，需要将LoRA权重与基础模型合并\n",
    "    from peft import PeftModel\n",
    "    # 重新加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # 加载LoRA适配器并与基础模型合并\n",
    "    model = PeftModel.from_pretrained(base_model, final_lora_path)\n",
    "    model = model.merge_and_unload() # 合并权重\n",
    "    \n",
    "\n",
    "    # 在测试集上进行预测并记录结果\n",
    "    print(\"在测试集上进行预测...\")\n",
    "    test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]  # 取前3条测试\n",
    "    test_text_list = []\n",
    "    \n",
    "    for index, row in test_df.iterrows():\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": row['instruction']},\n",
    "            {\"role\": \"user\", \"content\": row['input']}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = predict(messages, model, tokenizer)\n",
    "            response_text = f\"\"\"\n",
    "            样本 {index + 1}:\n",
    "            问题: {row['input']}\n",
    "            模型回答: {response}\n",
    "            \"\"\"\n",
    "            print(response_text)\n",
    "            test_text_list.append(swanlab.Text(response_text))\n",
    "        except Exception as e:\n",
    "            print(f\"预测样本 {index + 1} 时出错: {e}\")\n",
    "    \n",
    "    # 记录预测结果到swanlab\n",
    "    swanlab.log({\"预测结果\": test_text_list})\n",
    "    swanlab.finish()\n",
    "    print(\"训练和评估完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c579dd-c2db-4ad4-ae04-12e8f69ed8fa",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-08-15T02:44:07.966667Z",
     "iopub.status.busy": "2025-08-15T02:44:07.966398Z",
     "iopub.status.idle": "2025-08-15T03:10:40.183901Z",
     "shell.execute_reply": "2025-08-15T03:10:40.183246Z",
     "shell.execute_reply.started": "2025-08-15T02:44:07.966651Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc397df8c93d4490b63f757c16013f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回答：<|FunctionCallBegin|>嗯，用户问的是头痛的常见原因有哪些，以及如何根据不同的原因选择合适的药物。首先，我需要回忆一下常见的头痛类型，比如紧张性头痛、偏头痛、丛集性头痛，还有其他类型比如血管性头痛或者颅内压增高引起的。然后，针对每种头痛类型，常见的治疗方法是什么，药物上有什么区别。\n",
      "\n",
      "用户还提到自己头痛厉害，想知道吃什么药，所以可能需要先判断头痛的原因，再给出建议。但用户可能不太清楚症状的具体情况，所以需要先询问症状类型，比如是紧张型还是偏头痛，这样药物选择才能更准确。不过用户可能希望得到一般性的指导，所以需要平衡专业性和易懂性。\n",
      "\n",
      "接下来，我需要列出常见的头痛原因，确保覆盖主要类型。比如紧张性头痛通常与压力有关，可能需要非甾体抗炎药；偏头痛可能需要抗凝血药或者钙通道阻滞剂；丛集性头痛可能需要特定的药物如氟桂利嗪或曲普坦类。还有血管性头痛可能需要血管收缩剂或者钙通道调节剂。颅内压增高可能需要降颅压药物，但这种情况需要专业诊断。\n",
      "\n",
      "然后，根据不同的原因，药物的分类也要对应。比如，钙通道阻滞剂（如氟桂利嗪）常用于偏头痛和血管性头痛。抗凝血药如华法林用于偏头痛，因为偏头痛可能有血栓形成的风险。曲普坦类如舒马曲普坦用于急性偏头痛发作。血管扩张剂如氟桂利嗪可能用于血管性头痛。降颅压药物如甘露醇用于颅内压增高的情况。\n",
      "\n",
      "用户可能需要知道在什么情况下使用哪种药物，比如急性发作时用曲普坦，长期预防可能用氟桂利嗪。另外，如果症状严重或持续，应该尽快就医，避免自行用药延误治疗。\n",
      "\n",
      "还要注意用户可能没有明确说明症状的具体类型，所以回答中需要建议他们详细描述症状，以便医生判断。同时，强调药物的选择需要专业医生的指导，避免自行用药的风险。\n",
      "\n",
      "总结下来，回答的结构应该是先分类头痛原因，然后分点说明每种原因对应的药物，最后建议就医和描述症状。这样既专业又易懂，符合用户的需求。\n",
      "<|FunctionCallEnd|> \n",
      " 您好，头痛的原因有很多，比如紧张性头痛、偏头痛、丛集性头痛、血管性头痛、颅内压增高等。针对不同的原因，您可以选择相应的药物。例如，偏头痛发作时，可以使用钙通道阻滞剂或抗凝血药；血管性头痛可以使用血管扩张剂或钙通道调节剂；而颅内压增高则需要使用降颅压药物。不过，为了确保安全和有效，建议您详细描述您的症状，以便我们为您选择最适合的药物。如果头痛严重或持续不缓解，也请尽快就医，以便得到专业的诊断和治疗。<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "def predict_stream_simple(messages, model, tokenizer):\n",
    "    \"\"\"\n",
    "    流式预测函数：使用TextStreamer直接在控制台打印输出\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 应用聊天模板格式化输入\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 转换为模型输入格式\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 创建 TextStreamer\n",
    "    # skip_prompt=True 会跳过打印输入的部分，只打印模型生成的内容\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # 流式生成回应，这里会直接打印到屏幕\n",
    "    # 注意：generate函数现在会阻塞直到生成结束，但过程中会流式打印\n",
    "    _ = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        streamer=streamer # 关键参数：传入streamer\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 确保你的路径是正确的\n",
    "    final_lora_path = 'Qwen3-1.7B-LoRA/final_lora_adapter'\n",
    "    model_dir = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "    # 检查路径是否存在\n",
    "    if not os.path.exists(final_lora_path):\n",
    "        raise FileNotFoundError(f\"LoRA adapter path not found: {final_lora_path}\")\n",
    "    if not os.path.exists(model_dir):\n",
    "         raise FileNotFoundError(f\"Base model path not found: {model_dir}\")\n",
    "\n",
    "    # 重新加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    # 加载LoRA适配器并与基础模型合并\n",
    "    model = PeftModel.from_pretrained(base_model, final_lora_path)\n",
    "    model = model.merge_and_unload() # 合并权重\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    instruction = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "    question = \"你好，医生，我现在头痛的厉害，吃什么药可以缓解呢\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    print(\"回答：\", end=\"\", flush=True)\n",
    "    # 直接调用新的函数\n",
    "    predict_stream_simple(messages, model, tokenizer)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ab485-ad7e-4d58-936c-5359527b0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "def predict_and_stream(messages, model, tokenizer):\n",
    "    \"\"\"\n",
    "    修改后的流式预测函数：\n",
    "    1. 使用TextStreamer直接在控制台打印输出。\n",
    "    2. 返回模型生成的完整回复文本，用于添加到对话历史中。\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 应用聊天模板格式化输入\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 转换为模型输入格式\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    input_ids = model_inputs.input_ids\n",
    "    input_length = input_ids.shape[1] # 记录输入内容的长度\n",
    "\n",
    "    # 创建 TextStreamer\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    # 使用 streamer 进行流式输出，同时接收完整的生成结果\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    \n",
    "    # 从返回的 generated_ids 中解码出完整的回复\n",
    "    # 需要去掉输入部分，只保留新生成的部分\n",
    "    response_ids = generated_ids[0][input_length:]\n",
    "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 模型加载部分 (与之前相同) ---\n",
    "    final_lora_path = 'Qwen3-1.7B-LoRA/final_lora_adapter'\n",
    "    model_dir = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "    if not os.path.exists(final_lora_path):\n",
    "        raise FileNotFoundError(f\"LoRA adapter path not found: {final_lora_path}\")\n",
    "    if not os.path.exists(model_dir):\n",
    "         raise FileNotFoundError(f\"Base model path not found: {model_dir}\")\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, final_lora_path)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # --- 聊天循环部分 (核心修改) ---\n",
    "    \n",
    "    # 1. 初始化对话历史，包含系统指令\n",
    "    instruction = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction}\n",
    "    ]\n",
    "    \n",
    "    print(\"医学专家已上线。输入 'exit' 或 'quit' 退出程序。\")\n",
    "    \n",
    "    # 2. 启动一个无限循环来进行对话\n",
    "    while True:\n",
    "        # 获取用户输入\n",
    "        question = input(\"你: \")\n",
    "        \n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"感谢您的使用！\")\n",
    "            break\n",
    "            \n",
    "        # 3. 将用户的提问添加到历史记录\n",
    "        messages.append({\"role\": \"user\", \"content\": question})\n",
    "        \n",
    "        print(\"专家: \", end=\"\", flush=True)\n",
    "        \n",
    "        # 4. 调用模型，传入完整的对话历史\n",
    "        # 函数会流式打印答案，并返回完整的答案文本\n",
    "        assistant_response = predict_and_stream(messages, model, tokenizer)\n",
    "        \n",
    "        # 换行，以便下一次输入\n",
    "        print() \n",
    "        \n",
    "        # 5. 将模型的回答也添加到历史记录，为下一次对话做准备\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
